# YAML anchors for shared configuration
x-common-env: &common-env # Database Configuration
  DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:postgres@postgres:5432/supercheck}
  DB_HOST: ${DB_HOST:-postgres}
  DB_PORT: ${DB_PORT:-5432}
  DB_USER: ${DB_USER:-postgres}
  DB_PASSWORD: ${DB_PASSWORD:-postgres}
  DB_NAME: ${DB_NAME:-supercheck}

  # Redis Configuration
  REDIS_HOST: ${REDIS_HOST:-redis}
  REDIS_PORT: ${REDIS_PORT:-6379}
  REDIS_PASSWORD: ${REDIS_PASSWORD:-supersecure-redis-password-change-this}
  REDIS_URL: ${REDIS_URL:-redis://:supersecure-redis-password-change-this@redis:6379}

  # App Configuration
  NEXT_PUBLIC_APP_URL: ${NEXT_PUBLIC_APP_URL:-https://demo.supercheck.io}
  # Runtime env vars for middleware (middleware can't access NEXT_PUBLIC_ vars at runtime)
  APP_URL: https://demo.supercheck.io
  STATUS_PAGE_DOMAIN: supercheck.io
  BETTER_AUTH_URL: https://demo.supercheck.io
  BETTER_AUTH_SECRET: ${BETTER_AUTH_SECRET:-CHANGE_THIS_GENERATE_32_CHAR_HEX}
  NODE_ENV: ${NODE_ENV:-production}
  RUNNING_CAPACITY: ${RUNNING_CAPACITY:-3} # Optimized for 4-core server: 4 workers × 1.5 jobs per worker
  QUEUED_CAPACITY: ${QUEUED_CAPACITY:-10} # Max queued jobs before backpressure (default: 10)
  MAX_CONCURRENT_EXECUTIONS: ${MAX_CONCURRENT_EXECUTIONS:-2} # Max parallel tests per worker (default: 1, use worker replicas for parallelism)
  TEST_EXECUTION_TIMEOUT_MS: ${TEST_EXECUTION_TIMEOUT_MS:-120000}
  JOB_EXECUTION_TIMEOUT_MS: ${JOB_EXECUTION_TIMEOUT_MS:-900000}

  # AWS S3 / MinIO Configuration
  AWS_REGION: ${AWS_REGION:-us-east-1}
  AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
  AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
  S3_ENDPOINT: ${S3_ENDPOINT:-http://minio:9000}
  S3_JOB_BUCKET_NAME: ${S3_JOB_BUCKET_NAME:-playwright-job-artifacts}
  S3_TEST_BUCKET_NAME: ${S3_TEST_BUCKET_NAME:-playwright-test-artifacts}
  S3_MONITOR_BUCKET_NAME: ${S3_MONITOR_BUCKET_NAME:-playwright-monitor-artifacts}
  S3_STATUS_BUCKET_NAME: ${S3_STATUS_BUCKET_NAME:-supercheck-status-artifacts}
  S3_FORCE_PATH_STYLE: ${S3_FORCE_PATH_STYLE:-true}
  S3_OPERATION_TIMEOUT: ${S3_OPERATION_TIMEOUT:-10000}
  S3_MAX_RETRIES: ${S3_MAX_RETRIES:-3}

  # Notification Channel Limits
  NEXT_PUBLIC_MAX_JOB_NOTIFICATION_CHANNELS: ${NEXT_PUBLIC_MAX_JOB_NOTIFICATION_CHANNELS:-10}
  NEXT_PUBLIC_MAX_MONITOR_NOTIFICATION_CHANNELS: ${NEXT_PUBLIC_MAX_MONITOR_NOTIFICATION_CHANNELS:-10}

  # Playwright Configuration - Optimized for Supercheck execution service
  PLAYWRIGHT_HEADLESS: ${PLAYWRIGHT_HEADLESS:-true}
  PLAYWRIGHT_RETRIES: ${PLAYWRIGHT_RETRIES:-1}
  PLAYWRIGHT_TRACE: ${PLAYWRIGHT_TRACE:-retain-on-failure}
  PLAYWRIGHT_SCREENSHOT: ${PLAYWRIGHT_SCREENSHOT:-on}
  # Video recording enabled on test failures for debugging (with increased resource limits)
  PLAYWRIGHT_VIDEO: ${PLAYWRIGHT_VIDEO:-retain-on-failure}

  # Browser support (disabled by default for performance)
  ENABLE_FIREFOX: ${ENABLE_FIREFOX:-false}
  ENABLE_WEBKIT: ${ENABLE_WEBKIT:-false}
  ENABLE_MOBILE: ${ENABLE_MOBILE:-false}

  # Monitor Results Cleanup
  MONITOR_CLEANUP_ENABLED: ${MONITOR_CLEANUP_ENABLED:-true}
  MONITOR_CLEANUP_CRON: ${MONITOR_CLEANUP_CRON:-"0 2 * * *"} # 2 AM daily
  MONITOR_RETENTION_DAYS: ${MONITOR_RETENTION_DAYS:-30}
  MONITOR_CLEANUP_BATCH_SIZE: ${MONITOR_CLEANUP_BATCH_SIZE:-1000}
  MONITOR_PRESERVE_STATUS_CHANGES: ${MONITOR_PRESERVE_STATUS_CHANGES:-true}
  MONITOR_CLEANUP_SAFETY_LIMIT: ${MONITOR_CLEANUP_SAFETY_LIMIT:-1000000}

  # Job Runs Cleanup
  JOB_RUNS_CLEANUP_ENABLED: ${JOB_RUNS_CLEANUP_ENABLED:-false}
  JOB_RUNS_CLEANUP_CRON: ${JOB_RUNS_CLEANUP_CRON:-"0 3 * * *"} # 3 AM daily
  JOB_RUNS_RETENTION_DAYS: ${JOB_RUNS_RETENTION_DAYS:-90}
  JOB_RUNS_CLEANUP_BATCH_SIZE: ${JOB_RUNS_CLEANUP_BATCH_SIZE:-100}
  JOB_RUNS_CLEANUP_SAFETY_LIMIT: ${JOB_RUNS_CLEANUP_SAFETY_LIMIT:-10000}

  # Playground Artifacts Cleanup
  PLAYGROUND_CLEANUP_ENABLED: ${PLAYGROUND_CLEANUP_ENABLED:-true}
  PLAYGROUND_CLEANUP_CRON: ${PLAYGROUND_CLEANUP_CRON:-"0 */12 * * *"} # Every 12 hours
  PLAYGROUND_CLEANUP_MAX_AGE_HOURS: ${PLAYGROUND_CLEANUP_MAX_AGE_HOURS:-24}

  # Security Configuration
  SECRET_ENCRYPTION_KEY: ${SECRET_ENCRYPTION_KEY:-CHANGE_THIS_GENERATE_32_CHAR_HEX}

  # Better Auth Admin Configuration
  MAX_PROJECTS_PER_ORG: ${MAX_PROJECTS_PER_ORG:-10}
  DEFAULT_PROJECT_NAME: ${DEFAULT_PROJECT_NAME:-Default Project}

  # SMTP Email Configuration for Notifications
  # Supports any SMTP provider including Resend SMTP, Gmail, SendGrid, etc.
  SMTP_HOST: ${SMTP_HOST:-smtp.resend.com}
  SMTP_PORT: ${SMTP_PORT:-587}
  SMTP_USER: ${SMTP_USER:-resend}
  SMTP_PASSWORD: ${SMTP_PASSWORD:-your-smtp-password-change-this-in-production}
  SMTP_SECURE: ${SMTP_SECURE:-false}
  SMTP_FROM_EMAIL: ${SMTP_FROM_EMAIL:-notification@example.com}

  # AI Fix Feature Configuration
  AI_PROVIDER: ${AI_PROVIDER:-openai}
  AI_MODEL: ${AI_MODEL:-gpt-4o-mini}
  OPENAI_API_KEY: ${OPENAI_API_KEY:-your-openai-api-key-here}
  AI_TIMEOUT_MS: ${AI_TIMEOUT_MS:-90000}
  AI_MAX_RETRIES: ${AI_MAX_RETRIES:-2}
  AI_TEMPERATURE: ${AI_TEMPERATURE:-0.1}

  # Traefik/HTTPS Configuration
  APP_DOMAIN: demo.supercheck.io
  ACME_EMAIL: hello@supercheck.io

  NEXT_PUBLIC_DEMO_MODE: ${NEXT_PUBLIC_DEMO_MODE:-true} # Set to true to enable demo mode

services:
  # Traefik Reverse Proxy with HTTPS
  traefik:
    image: traefik:v3.0
    environment:
      # Traefik Configuration using environment variables
      - TRAEFIK_API_DASHBOARD=false
      - TRAEFIK_PROVIDERS_DOCKER=true
      - TRAEFIK_PROVIDERS_DOCKER_EXPOSEDBYDEFAULT=false
      - TRAEFIK_ENTRYPOINTS_WEB_ADDRESS=:80
      - TRAEFIK_ENTRYPOINTS_WEBSECURE_ADDRESS=:443
      # Disable Let's Encrypt since Cloudflare handles SSL termination
      - TRAEFIK_ENTRYPOINTS_WEBSECURE_HTTP_TLS_OPTIONS=default
      - TRAEFIK_SERVERSTRANSPORT_INSECURESKIPVERIFY=true
      - TRAEFIK_LOG_LEVEL=INFO
      - TRAEFIK_ACCESSLOG=true
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik-letsencrypt:/letsencrypt
    labels:
      - "traefik.enable=false"
    networks:
      - supercheck-network
    restart: unless-stopped

  # App (Next.js Frontend) - Handles migrations internally
  app:
    image: ghcr.io/supercheck-io/supercheck/app:1.1.5-beta.17
    expose:
      - "3000"
    environment:
      <<: *common-env
      # App-specific configuration (inherits all common env vars)
    volumes:
      - ./app/public/tests:/app/public/tests
      - ./app/public/test-results:/app/public/test-results
      - ./app/public/artifacts:/app/public/artifacts
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "const http = require('http'); const req = http.request({hostname: 'localhost', port: 3000, path: '/', timeout: 5000}, (res) => process.exit(res.statusCode < 400 ? 0 : 1)); req.on('error', () => process.exit(1)); req.end();",
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s # Increased to allow for migrations
    labels:
      - "traefik.enable=true"

      # Main app — exact host (demo.supercheck.io or override with APP_DOMAIN env var)
      - "traefik.http.routers.app.rule=Host(`demo.supercheck.io`)"
      - "traefik.http.routers.app.priority=100"
      - "traefik.http.routers.app.entrypoints=websecure"
      - "traefik.http.routers.app.tls=true"
      - "traefik.http.routers.app.service=app"

      # Status pages — any subdomain under supercheck.io (priority 50 to match after main app)
      # Traefik v3 HostRegexp: match alphanumeric subdomains (UUIDs)
      # Main app router handles demo.supercheck.io first (higher priority)
      - "traefik.http.routers.status-pages.rule=HostRegexp(`[a-zA-Z0-9-]+\\.supercheck\\.io`)"
      - "traefik.http.routers.status-pages.priority=50"
      - "traefik.http.routers.status-pages.entrypoints=websecure"
      - "traefik.http.routers.status-pages.tls=true"
      - "traefik.http.routers.status-pages.service=app"

      # HTTP → HTTPS redirect for main app
      - "traefik.http.routers.app-http.rule=Host(`demo.supercheck.io`)"
      - "traefik.http.routers.app-http.entrypoints=web"
      - "traefik.http.routers.app-http.middlewares=app-https-redirect"
      - "traefik.http.routers.app-http.service=app"

      # HTTP → HTTPS redirect for status pages
      - "traefik.http.routers.status-pages-http.rule=HostRegexp(`[a-zA-Z0-9-]+\\.supercheck\\.io`)"
      - "traefik.http.routers.status-pages-http.entrypoints=web"
      - "traefik.http.routers.status-pages-http.middlewares=app-https-redirect"
      - "traefik.http.routers.status-pages-http.service=app"

      # Redirect middleware & service config
      - "traefik.http.middlewares.app-https-redirect.redirectscheme.scheme=https"
      - "traefik.http.services.app.loadbalancer.server.port=3000"
      - "traefik.http.services.app.loadbalancer.healthCheck.path=/api/health"
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 1G

  # Worker (NestJS Runner) - Minimal resource limits for maximum test compatibility
  worker:
    image: ghcr.io/supercheck-io/supercheck/worker:1.1.5-beta.17
    environment:
      <<: *common-env
      # Worker-specific configuration (inherits all common env vars)
      # Removed NODE_OPTIONS memory limits - let Node.js use what it needs
      # Removed UV_THREADPOOL_SIZE limits - let system manage threads

    volumes:
      - worker-playwright-reports:/app/playwright-reports
      - worker-reports:/app/report
      # CRITICAL: Use host's shared memory - no artificial size limit
      - type: tmpfs
        target: /dev/shm
        tmpfs:
          size: 8589934592 # 8GB shared memory (increased from 3GB to prevent browser crashes)
    depends_on:
      postgres:
        condition: service_healthy
      app:
        condition: service_healthy # Wait for app to complete migrations
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "node",
          "-e",
          "const http = require('http'); const req = http.request({hostname: 'localhost', port: 3001, path: '/health', timeout: 5000}, (res) => process.exit(res.statusCode === 200 ? 0 : 1)); req.on('error', () => process.exit(1)); req.end();",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - supercheck-network
    deploy:
      replicas: 3 # Balanced for performance and stability
      # REMOVED RESOURCE LIMITS - Let Docker and the host OS manage resources
      # Docker will kill the container if it goes rogue (OOM killer, etc.)
      # This prevents artificial constraints from breaking legitimate tests

      # Container Restart Protection (handles stuck/crashed containers)
      restart_policy:
        condition: on-failure
        max_attempts: 3
        delay: 15s
        window: 120s

    # Minimal system-level settings for compatibility
    sysctls:
      - net.core.somaxconn=4096 # High connection backlog for Playwright
    ulimits:
      nproc: 65535 # Allow many processes (browser + video + FFmpeg)
      nofile: # High file descriptor limits for browser tabs
        soft: 1048576
        hard: 1048576
      memlock: # Unlimited memory locking for browser
        soft: -1
        hard: -1
    # Security constraints (minimal - only prevent privilege escalation)
    security_opt:
      - no-new-privileges:true # Prevent privilege escalation
    cap_drop:
      - ALL # Drop all capabilities by default
    cap_add:
      - SYS_ADMIN # Required for browser sandboxing (minimal required capability)

  # PostgreSQL Database
  postgres:
    image: postgres:18
    environment:
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
      - POSTGRES_DB=${DB_NAME:-supercheck}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 15
      start_period: 30s
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1.5G
        reservations:
          cpus: "0.5"
          memory: 1G

  # Redis (for job queues)
  redis:
    image: redis:8
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-supersecure-redis-password-change-this}
    command: sh -c "rm -rf /data/* && redis-server --requirepass \"${REDIS_PASSWORD:-supersecure-redis-password-change-this}\" --maxmemory 512mb --maxmemory-policy noeviction --save '' --appendonly no --protected-mode yes --bind 0.0.0.0 --tcp-keepalive 300"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "redis-cli",
          "-a",
          "${REDIS_PASSWORD:-supersecure-redis-password-change-this}",
          "ping",
        ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
        reservations:
          cpus: "0.25"
          memory: 256M

  # MinIO (S3-compatible storage)
  minio:
    image: minio/minio:latest
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID:-minioadmin}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY:-minioadmin}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - supercheck-network
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 1G
        reservations:
          cpus: "0.25"
          memory: 512M

volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  minio-data:
    driver: local
  worker-playwright-reports:
    driver: local
  worker-reports:
    driver: local
  traefik-letsencrypt:
    driver: local

networks:
  supercheck-network:
    driver: bridge
