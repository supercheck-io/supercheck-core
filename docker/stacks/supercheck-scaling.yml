version: "3.8"

services:
  # Supercheck App with High Availability for paid plans
  supercheck-app:
    image: ghcr.io/supercheck-io/supercheck/app:1.0.5-beta
    environment:
      # Database Configuration (External - Neon/PlanetScale)
      - DATABASE_URL_FILE=/run/secrets/database_url
      - DB_SSL_MODE=require
      - DB_CONNECTION_LIMIT=50  # Increased for paid plans
      - DB_POOL_TIMEOUT=30000

      # Redis Configuration (External - Redis Cloud)
      - REDIS_URL_FILE=/run/secrets/redis_url
      - REDIS_CONNECTION_TIMEOUT=5000
      - REDIS_COMMAND_TIMEOUT=5000
      - REDIS_MAX_RETRIES=3
      - REDIS_POOL_SIZE=20  # Increased for concurrent job management

      # External S3 Configuration
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID_FILE=/run/secrets/aws_access_key_id
      - AWS_SECRET_ACCESS_KEY_FILE=/run/secrets/aws_secret_access_key
      - S3_JOB_BUCKET_NAME=supercheck-job-artifacts
      - S3_TEST_BUCKET_NAME=supercheck-test-artifacts
      - S3_FORCE_PATH_STYLE=false
      - S3_OPERATION_TIMEOUT=10000
      - S3_MAX_RETRIES=3

      # Scaling Configuration for Paid Plans
      - NODE_ENV=production
      - NEXT_PUBLIC_APP_URL=https://supercheck.yourdomain.com
      - BETTER_AUTH_URL=https://supercheck.yourdomain.com
      - BETTER_AUTH_SECRET_FILE=/run/secrets/auth_secret

      # Increased capacity for paid plans
      - RUNNING_CAPACITY=2500  # Support 2500 concurrent tests
      - QUEUED_CAPACITY=5000   # Queue up to 5000 tests
      - TEST_EXECUTION_TIMEOUT_MS=300000  # 5 minutes max per test
      - JOB_EXECUTION_TIMEOUT_MS=1800000  # 30 minutes max per job

      # Multi-tenant Configuration
      - MAX_CONCURRENT_TESTS_BASIC=5      # Basic plan limit
      - MAX_CONCURRENT_TESTS_PRO=20       # Pro plan limit
      - MAX_CONCURRENT_TESTS_ENTERPRISE=100  # Enterprise plan limit

      # Resource Management
      - WORKER_SCALING_ENABLED=true
      - AUTO_SCALE_THRESHOLD=80  # Scale when 80% capacity used
      - SCALE_UP_COOLDOWN=60     # Wait 1 minute before scaling up
      - SCALE_DOWN_COOLDOWN=300  # Wait 5 minutes before scaling down

      # Performance Configuration
      - CREDENTIAL_ENCRYPTION_KEY_FILE=/run/secrets/credential_encryption_key
      - VARIABLES_ENCRYPTION_KEY_FILE=/run/secrets/variables_encryption_key

      # Monitoring and Analytics for Paid Plans
      - ENABLE_USAGE_ANALYTICS=true
      - ENABLE_PERFORMANCE_METRICS=true
      - BILLING_INTEGRATION_ENABLED=true

    secrets:
      - database_url
      - redis_url
      - aws_access_key_id
      - aws_secret_access_key
      - auth_secret
      - credential_encryption_key
      - variables_encryption_key
    ports:
      - "3000:3000"
    networks:
      - supercheck-network
    deploy:
      replicas: 3  # High availability for paid service
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone  # Spread across zones
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
      rollback_config:
        parallelism: 1
        delay: 10s
        failure_action: pause
        monitor: 60s
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    healthcheck:
      test: ["CMD", "node", "-e", "const http = require('http'); const req = http.request({hostname: 'localhost', port: 3000, path: '/api/health', timeout: 5000}, (res) => process.exit(res.statusCode < 400 ? 0 : 1)); req.on('error', () => process.exit(1)); req.end();"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
        labels: "service,environment,plan"
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.supercheck-app.rule=Host(`supercheck.yourdomain.com`)"
      - "traefik.http.routers.supercheck-app.entrypoints=websecure"
      - "traefik.http.routers.supercheck-app.tls.certresolver=letsencrypt"
      - "traefik.http.services.supercheck-app.loadbalancer.server.port=3000"
      - "traefik.http.services.supercheck-app.loadbalancer.healthcheck.path=/api/health"
      - "traefik.docker.network=supercheck-network"

  # Supercheck Worker - Optimized for Concurrent Test Execution
  supercheck-worker:
    image: ghcr.io/supercheck-io/supercheck/worker:1.0.5-beta
    environment:
      # Database Configuration (Connection pooling optimized)
      - DATABASE_URL_FILE=/run/secrets/database_url
      - DB_SSL_MODE=require
      - DB_CONNECTION_LIMIT=10  # Per worker instance
      - DB_POOL_TIMEOUT=30000

      # Redis Configuration (Job queue management)
      - REDIS_URL_FILE=/run/secrets/redis_url
      - REDIS_CONNECTION_TIMEOUT=5000
      - REDIS_COMMAND_TIMEOUT=5000
      - REDIS_MAX_RETRIES=3
      - REDIS_POOL_SIZE=10

      # S3 Configuration (Artifact storage)
      - AWS_REGION=us-east-1
      - AWS_ACCESS_KEY_ID_FILE=/run/secrets/aws_access_key_id
      - AWS_SECRET_ACCESS_KEY_FILE=/run/secrets/aws_secret_access_key
      - S3_JOB_BUCKET_NAME=supercheck-job-artifacts
      - S3_TEST_BUCKET_NAME=supercheck-test-artifacts
      - S3_FORCE_PATH_STYLE=false
      - S3_OPERATION_TIMEOUT=10000
      - S3_MAX_RETRIES=3
      - S3_MULTIPART_THRESHOLD=10485760  # 10MB
      - S3_MULTIPART_CHUNKSIZE=5242880   # 5MB

      # Worker-specific Configuration for Paid Plans
      - NODE_ENV=production
      - NODE_OPTIONS=--max-old-space-size=2048 --expose-gc --experimental-worker
      - UV_THREADPOOL_SIZE=8  # Increased for concurrent processing

      # Concurrent Test Management
      - MAX_CONCURRENT_TESTS_PER_WORKER=5  # 5 concurrent tests per worker
      - TEST_ISOLATION_ENABLED=true        # Isolate tests from each other
      - BROWSER_INSTANCE_REUSE=false       # Fresh browser per test
      - WORKER_ID=${HOSTNAME}              # Unique worker identification

      # Performance Configuration
      - TEST_EXECUTION_TIMEOUT_MS=300000   # 5 minutes max
      - JOB_EXECUTION_TIMEOUT_MS=1800000   # 30 minutes max
      - MEMORY_CLEANUP_INTERVAL=30000      # Clean up every 30 seconds
      - FORCE_GC_INTERVAL=60000            # Force garbage collection every minute

      # Playwright Configuration for Paid Plans
      - PLAYWRIGHT_HEADLESS=true
      - PLAYWRIGHT_RETRIES=1
      - PLAYWRIGHT_TRACE=on
      - PLAYWRIGHT_SCREENSHOT=on
      - PLAYWRIGHT_VIDEO=on
      - PLAYWRIGHT_BROWSER_TIMEOUT=60000
      - PLAYWRIGHT_NAVIGATION_TIMEOUT=30000
      - ENABLE_FIREFOX=false
      - ENABLE_WEBKIT=false
      - ENABLE_MOBILE=false

      # Resource Management
      - ENABLE_MEMORY_MONITORING=true
      - MAX_MEMORY_USAGE_MB=1800  # Leave 200MB buffer
      - RESTART_ON_MEMORY_LEAK=true

    secrets:
      - database_url
      - redis_url
      - aws_access_key_id
      - aws_secret_access_key
    networks:
      - supercheck-network
    deploy:
      replicas: 20  # Start with 20 workers (100 concurrent tests)
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
      update_config:
        parallelism: 5  # Update 5 workers at a time
        delay: 15s
        failure_action: rollback
        monitor: 30s
      rollback_config:
        parallelism: 2
        delay: 5s
        failure_action: pause
        monitor: 30s
      resources:
        limits:
          cpus: '2.0'    # 2 CPU cores per worker
          memory: 2G     # 2GB RAM per worker
          pids: 4096     # Limit processes to prevent fork bombs
        reservations:
          cpus: '1.0'    # Reserve 1 CPU core
          memory: 1G     # Reserve 1GB RAM
    healthcheck:
      test: ["CMD", "node", "-e", "const http = require('http'); const req = http.request({hostname: 'localhost', port: 3001, path: '/health', timeout: 5000}, (res) => process.exit(res.statusCode === 200 ? 0 : 1)); req.on('error', () => process.exit(1)); req.end();"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,worker_id,plan"
    volumes:
      - /tmp:/tmp  # Shared tmp for Playwright
    # Security context for test isolation
    user: "1000:1000"  # Non-root user
    # Anti-affinity to spread workers across nodes
    deploy:
      placement:
        preferences:
          - spread: node.id

  # Traefik with Rate Limiting for Paid Plans
  traefik:
    image: traefik:v3.0
    command:
      # Docker Swarm provider
      - --providers.swarm.endpoint=unix:///var/run/docker.sock
      - --providers.swarm.exposedByDefault=false
      - --providers.swarm.network=supercheck-network

      # Entry points
      - --entrypoints.web.address=:80
      - --entrypoints.websecure.address=:443
      - --entrypoints.web.http.redirections.entrypoint.to=websecure
      - --entrypoints.web.http.redirections.entrypoint.scheme=https

      # Let's Encrypt
      - --certificatesresolvers.letsencrypt.acme.httpchallenge=true
      - --certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web
      - --certificatesresolvers.letsencrypt.acme.email=admin@yourdomain.com
      - --certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json

      # API and Dashboard
      - --api.dashboard=true
      - --api.insecure=false

      # Metrics for paid plan monitoring
      - --metrics.prometheus=true
      - --metrics.prometheus.addEntryPointsLabels=true
      - --metrics.prometheus.addServicesLabels=true

      # Rate limiting for paid plans
      - --pilot.dashboard=false

      # Logging
      - --log.level=INFO
      - --accesslog=true
      - --accesslog.format=json
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik-letsencrypt:/letsencrypt
    networks:
      - supercheck-network
    deploy:
      replicas: 2  # High availability for paid service
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.traefik.rule=Host(`traefik.yourdomain.com`)"
        - "traefik.http.routers.traefik.entrypoints=websecure"
        - "traefik.http.routers.traefik.tls.certresolver=letsencrypt"
        - "traefik.http.routers.traefik.service=api@internal"
        - "traefik.http.services.traefik.loadbalancer.server.port=8080"

# Docker Secrets for External Services
secrets:
  database_url:
    external: true
  redis_url:
    external: true
  aws_access_key_id:
    external: true
  aws_secret_access_key:
    external: true
  auth_secret:
    external: true
  credential_encryption_key:
    external: true
  variables_encryption_key:
    external: true

# Networks
networks:
  supercheck-network:
    driver: overlay
    attachable: true
    encrypted: true  # Encrypted for paid plan security
    ipam:
      config:
        - subnet: 10.0.9.0/24

# Volumes
volumes:
  traefik-letsencrypt:
    driver: local